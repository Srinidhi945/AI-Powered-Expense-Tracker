{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM///8DxUNcxqBd1bwlZ+iS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_community langchain-google-genai google-generativeai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T9JKW_a3twLv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get('google_api_key')\n",
        "langchain_api=userdata.get('langchain_api')"
      ],
      "metadata": {
        "id": "Rkn31kN8wCsp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PfK82gIitL5Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"]=google_api_key\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=langchain_api\n",
        "os.environ[\"LANGSMITH_PROJECT\"]=\"pr-only-pricing-57\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai  import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "q9u2arpMwZ8d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b8486d3",
        "outputId": "e15fe5d5-4ca3-442b-f6cb-3f11f3755d7d"
      },
      "source": [
        "!pip install -q redis"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/278.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.6/278.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.7/278.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REDIS_URL=\"redis://default:ZMnZepbuwK7EByHJtkhvjdYyWNz5bXCA@redis-13966.c11.us-east-1-2.ec2.redns.redis-cloud.com:13966\"\n",
        "def get_session_history(session_id:str):\n",
        "    return RedisChatMessageHistory(\n",
        "        session_id=session_id,\n",
        "        url=REDIS_URL\n",
        "    )\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\",convert_system_message_to_human=True)\n",
        "parser=StrOutputParser()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PXw5LkTrv0S-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def invoke_with_history(inputs):\n",
        "  history=inputs[\"history\"]\n",
        "  user_message=inputs[\"input\"]\n",
        "  messages=history+[user_message]\n",
        "  return llm.invoke(messages)\n",
        "chain=RunnableLambda(invoke_with_history)|parser"
      ],
      "metadata": {
        "id": "Z9QMBUpRsNDY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable=RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "MjXLL0idsBvs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user00\"\n",
        "while True:\n",
        "  user_input =input(\"You: \")\n",
        "  if user_input.lower() in [\"bye\", \"exit\"]:\n",
        "    print(\"Bot: Goodbye!\")\n",
        "    break\n",
        "\n",
        "  response =runnable.invoke(\n",
        "          {\"input\": HumanMessage(content=user_input)},\n",
        "          config={\"configurable\": {\"session_id\": session_id}}\n",
        "  )\n",
        "  print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32LBuQhisXIn",
        "outputId": "a043011b-c9e7-4df0-c762-6deae6302abf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hii i am nidhi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hi Nidhi! It's nice to meet you.\n",
            "\n",
            "How can I help you today?\n",
            "You: i what know todays Indian news headlines\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Of course, Nidhi. Here are some of the major news headlines from across India for today, June 19, 2024:\n",
            "\n",
            "**National & Political News**\n",
            "\n",
            "*   **NEET-UG Controversy:** The Supreme Court has issued a notice to the National Testing Agency (NTA) and the Centre regarding alleged malpractices in the NEET-UG 2024 exam, stating that even \"0.001% negligence\" should be dealt with. The court is hearing multiple petitions seeking a re-exam and a CBI probe.\n",
            "*   **Kanchanjunga Express Accident:** The death toll in the West Bengal train accident has risen to 10. The Commissioner of Railway Safety has started an inquiry into the cause of the collision between the goods train and the Sealdah-bound Kanchanjunga Express.\n",
            "*   **Parliament Session Preparations:** Ahead of the first Parliament session of the 18th Lok Sabha starting on June 24, political parties are finalizing their strategies. The election of the Speaker and the President's address will be key events.\n",
            "*   **Heatwave in North India:** A severe heatwave continues to grip North India, with Delhi and surrounding regions experiencing record-high temperatures. The India Meteorological Department (IMD) has issued a red alert for several states.\n",
            "\n",
            "**Business & Economy**\n",
            "\n",
            "*   **Stock Market Update:** Indian benchmark indices, Sensex and Nifty 50, opened higher today, hitting fresh all-time highs in early trade, driven by gains in banking and IT stocks.\n",
            "*   **GST Council Meeting:** The GST Council is scheduled to meet on June 22nd. The agenda is expected to include discussions on rate rationalization and clarification on the GST levy on online gaming.\n",
            "\n",
            "**Sports**\n",
            "\n",
            "*   **T20 World Cup 2024:** Team India is preparing for its first Super 8 match against Afghanistan, which is scheduled for tomorrow (June 20th) in Barbados.\n",
            "\n",
            "**International**\n",
            "\n",
            "*   **India-Sri Lanka Relations:** Sri Lankan President Ranil Wickremesinghe has stated that the implementation of the India-backed Kankesanthurai port development project will begin shortly.\n",
            "\n",
            "---\n",
            "\n",
            "Would you like me to elaborate on any of these headlines or find news on a specific topic for you?\n",
            "You: no need lets talk about the  demand for Datascience graduates in india\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Excellent topic, Nidhi! It's one of the most talked-about career fields in India right now.\n",
            "\n",
            "The demand for Data Science graduates in India is not just high; it's **explosive and consistently growing**. It's moved from being a niche specialty to a core function in many businesses.\n",
            "\n",
            "Let's break down why this is happening and what it means for graduates.\n",
            "\n",
            "### 1. Why is the Demand So High? (The Driving Forces)\n",
            "\n",
            "Several factors have created a \"perfect storm\" for data science in India:\n",
            "\n",
            "*   **The Data Explosion:** With one of the world's largest internet user bases, cheap data plans, and high smartphone penetration, Indians are generating an unimaginable amount of data every second. Companies see this data as a goldmine.\n",
            "*   **Digital India Initiative:** The government's push for digitization across all sectors (finance, healthcare, governance) has created massive datasets that need skilled professionals to analyze.\n",
            "*   **Booming Startup Ecosystem:** Indian startups, especially in e-commerce (Flipkart, Myntra), fintech (Paytm, Razorpay), and ed-tech (BYJU'S, Unacademy), are built on data. Their entire business model depends on understanding user behavior, personalizing recommendations, and optimizing operations.\n",
            "*   **Adoption by Traditional Industries:** It's not just tech companies anymore. Banks (like HDFC, ICICI) use data science for fraud detection and credit risk analysis. Retail companies (like Reliance Retail) use it for supply chain management and inventory forecasting.\n",
            "*   **Global Capability Centers (GCCs):** Many multinational corporations have set up their analytics and data science hubs in India to leverage the talent pool, creating a huge number of high-quality jobs.\n",
            "\n",
            "### 2. What Kind of Roles are in Demand?\n",
            "\n",
            "When we say \"Data Science,\" it's an umbrella term. For graduates, the opportunities usually fall into these categories:\n",
            "\n",
            "*   **Data Analyst:** This is often the entry point. They clean, interpret, and visualize data to answer specific business questions. **(Highest number of openings for freshers)**.\n",
            "*   **Data Scientist:** They use advanced statistics and machine learning to build predictive models. (e.g., \"Which customers are likely to churn?\").\n",
            "*   **Machine Learning (ML) Engineer:** They are software engineers who specialize in taking the models built by data scientists and deploying them into production-level applications.\n",
            "*   **Data Engineer:** They build the \"pipelines\" and infrastructure to collect, store, and process large datasets, making them available for analysts and scientists. This role is in extremely high demand.\n",
            "*   **BI (Business Intelligence) Analyst:** They focus on creating dashboards and reports (using tools like Tableau or Power BI) to help business leaders make decisions.\n",
            "\n",
            "### 3. The Scenario for Fresh Graduates: The Good and The Challenge\n",
            "\n",
            "**The Good News:**\n",
            "Companies are actively hiring fresh talent. They understand the need to build a pipeline of skilled professionals and are willing to invest in training. There are numerous \"Graduate Trainee\" or \"Junior Analyst\" roles.\n",
            "\n",
            "**The Challenge:**\n",
            "*   **High Competition:** Because it's a popular field, the number of applicants for each role is very high.\n",
            "*   **The Skills Gap:** A simple college degree is often not enough. Companies look for **practical, demonstrable skills**.\n",
            "\n",
            "### 4. How Can a Graduate Stand Out?\n",
            "\n",
            "This is the most crucial part. To be in high demand, a graduate needs to have:\n",
            "\n",
            "1.  **Strong Fundamentals:** A solid grasp of Statistics, Probability, and Linear Algebra.\n",
            "2.  **Technical Skills:**\n",
            "    *   **Programming:** Proficiency in **Python** (with libraries like Pandas, NumPy, Scikit-learn) is a must. R is also valued.\n",
            "    *   **Databases:** Knowledge of **SQL** is non-negotiable. It's the language of data.\n",
            "    *   **Data Visualization:** Experience with tools like **Tableau**, **Power BI**, or Python libraries (Matplotlib, Seaborn).\n",
            "3.  **Practical Experience (The Game Changer):**\n",
            "    *   **Projects:** Build a portfolio of 2-3 solid projects on platforms like **GitHub**. This is your proof of skill. Use real-world datasets from Kaggle or other public sources.\n",
            "    *   **Internships:** An internship is the single best way to get real-world experience and a potential job offer.\n",
            "    *   **Certifications:** While not a replacement for skills, certifications from reputable platforms (like Coursera, edX, or from cloud providers like AWS, Azure) can help your resume get noticed.\n",
            "\n",
            "### 5. Salary Expectations\n",
            "\n",
            "For a fresh graduate from a decent college with a good project portfolio, the starting salary in India typically ranges from **â‚¹5 lakhs to â‚¹12 lakhs per annum**. This can go significantly higher (â‚¹15-20 LPA+) for graduates from top-tier institutes (IITs, NITs, IIITs) or those hired by top product-based companies and MNCs.\n",
            "\n",
            "With 3-5 years of experience, this figure can easily double or triple.\n",
            "\n",
            "**In summary, Nidhi:** The demand for data science graduates in India is massive and real. However, the demand is for **skilled** graduates. The focus should be less on just getting a degree and more on building a practical, hands-on skill set and a strong project portfolio. It's a field that rewards continuous learning, but the career opportunities are truly fantastic.\n",
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user01\"\n",
        "while True:\n",
        "  user_input =input(\"You: \")\n",
        "  if user_input.lower() in [\"bye\", \"exit\"]:\n",
        "    print(\"Bot: Goodbye!\")\n",
        "    break\n",
        "\n",
        "  response =runnable.invoke(\n",
        "          {\"input\": HumanMessage(content=user_input)},\n",
        "          config={\"configurable\": {\"session_id\": session_id}}\n",
        "  )\n",
        "  print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VroAcNgct9F2",
        "outputId": "8d025140-5b08-43a7-ef75-764e08c9d37a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: hey i am sneha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hi Sneha! It's nice to meet you.\n",
            "\n",
            "How can I help you today?\n",
            "You: i am trying to build a project i.e AI research companion can u help me understand what it is i am asked to develop a rag application or so \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Of course, Sneha! That's a fantastic project. An \"AI Research Companion\" is a perfect use case for a **RAG application**. Let's break down exactly what that means.\n",
            "\n",
            "### What is a RAG Application?\n",
            "\n",
            "First, let's demystify the term.\n",
            "\n",
            "**RAG** stands for **R**etrieval-**A**ugmented **G**eneration.\n",
            "\n",
            "Think of a standard Large Language Model (LLM) like ChatGPT. It's like a brilliant student who has read a massive portion of the internet up to a certain date (e.g., 2023) and is now taking a **closed-book exam**. It can only answer questions based on the knowledge it has memorized.\n",
            "\n",
            "A **RAG application** is like giving that same brilliant student an **open-book exam**.\n",
            "\n",
            "Before answering your question, the AI gets to first **\"retrieve\"** relevant information from a specific set of documents (the \"book\") and then uses that information to **\"generate\"** a much more accurate and context-aware answer.\n",
            "\n",
            "---\n",
            "\n",
            "### Why is RAG Perfect for an \"AI Research Companion\"?\n",
            "\n",
            "Your goal is to help a user with AI research. This means you need your application to be:\n",
            "\n",
            "1.  **Accurate and Factual:** Standard LLMs can \"hallucinate\" or make up facts. A research tool cannot do that. RAG grounds the AI's answer in real documents (the research papers), drastically reducing hallucinations.\n",
            "2.  **Up-to-date:** AI research moves incredibly fast. A standard LLM's knowledge is frozen in time. With RAG, you can constantly add the latest research papers to your \"book\" (your knowledge base), so your companion is always current.\n",
            "3.  **Able to Cite Sources:** This is critical for any research tool. Because the RAG process starts by finding source documents, it's easy to show the user exactly where the information came from.\n",
            "\n",
            "So, your **AI Research Companion** will be an application that allows a user to ask questions in natural language and get answers that are synthesized directly from a library of AI research papers that you provide.\n",
            "\n",
            "---\n",
            "\n",
            "### How Does a RAG Application Work? (A Step-by-Step Flow)\n",
            "\n",
            "Let's imagine you have a folder full of 100 AI research papers in PDF format. Here's how your RAG app would work.\n",
            "\n",
            "#### Phase 1: The \"Library\" Phase (Indexing - Done once upfront)\n",
            "\n",
            "You need to prepare your documents so the AI can search them efficiently.\n",
            "\n",
            "1.  **Load & Chunk:** You can't feed a 20-page PDF to an AI at once. So, you load the papers and break them down into smaller, manageable \"chunks\" (e.g., paragraphs or a few paragraphs together).\n",
            "2.  **Create Embeddings:** This is the magic step. For each chunk of text, you use a special AI model (an Embedding Model) to convert it into a list of numbers called a **vector embedding**. Think of this vector as a \"GPS coordinate for meaning.\" Chunks with similar meanings will have similar coordinates.\n",
            "3.  **Store in a Vector Database:** You store all these vector embeddings (and the original text chunks they represent) in a special kind of database called a **Vector Database**. This database is highly optimized for finding the most similar vectors very, very quickly. Your \"library\" is now indexed and ready.\n",
            "You: okay can u suggest the best yet feasible free souced tech stack for this project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Excellent question. Choosing the right tech stack is crucial. You want something that is powerful, well-supported, and won't cost you anything to get started.\n",
            "\n",
            "Here is a recommended stack that is **100% free, open-source, and highly feasible** for building your AI Research Companion. It's one of the most popular and well-documented combinations right now.\n",
            "\n",
            "### The Recommended \"Feasible & Free\" Tech Stack\n",
            "\n",
            "| Component | Recommendation | Why it's a great choice |\n",
            "| :--- | :--- | :--- |\n",
            "| **Core Framework** | ðŸ¦œðŸ”— **LangChain** | The \"glue\" that connects everything. It simplifies the entire RAG pipeline, has a massive community, and tons of examples. |\n",
            "| **Document Loader** | LangChain's `PyPDFLoader` | Built directly into LangChain. It's specifically designed to load and parse text from PDF files, which is perfect for research papers. |\n",
            "| **Embedding Model** | `all-MiniLM-L6-v2` | A very popular, lightweight, and high-quality model from Hugging Face. Crucially, it's small enough to run quickly on a standard laptop CPU. |\n",
            "| **Vector Database** | **ChromaDB** | An open-source vector database that is incredibly easy to set up. It can run \"in-memory\" (no server needed) right inside your Python script, making it perfect for development. |\n",
            "| **LLM (Generator)** | **Groq** with Llama 3 | This is the secret weapon for getting **free, high-speed, top-tier model performance**. Groq offers an API that runs open-source models (like Meta's new Llama 3) at incredible speeds, and they have a very generous free tier. |\n",
            "\n",
            "---\n",
            "\n",
            "### Let's Break Down Each Choice\n",
            "\n",
            "#### 1. Core Framework: LangChain\n",
            "This is the conductor of your orchestra. Instead of you manually writing code to load a doc, chunk it, embed it, search the DB, and then call the LLM, LangChain provides high-level abstractions to do this in just a few lines of code.\n",
            "*   **Alternative:** **LlamaIndex**. It's also fantastic and is built specifically for RAG. For a pure research companion, it's an equally good choice. LangChain is just slightly more general-purpose.\n",
            "\n",
            "#### 2. Embedding Model: `all-MiniLM-L6-v2` (via SentenceTransformers)\n",
            "To use this, you'll install the `sentence-transformers` library from Hugging Face. LangChain has a built-in wrapper for it called `HuggingFaceEmbeddings`. This model is the best starting point because it balances performance and resource requirements perfectly. You don't need a powerful GPU.\n",
            "\n",
            "#### 3. Vector Database: ChromaDB\n",
            "Simplicity is key when you're starting. With Chroma, you don't need to install a separate database server or use Docker. You `pip install chromadb`, and it just works. It saves the vector index to a folder on your computer.\n",
            "*   **Alternative:** **FAISS** (from Facebook AI). It's a library, not a full DB, and is also very fast and runs locally. Chroma is a bit more user-friendly.\n",
            "\n",
            "#### 4. LLM (The \"Brain\"): Groq API\n",
            "This is the most challenging part of a \"free\" stack because running a good LLM locally requires a powerful (and expensive) GPU. The Groq API is the perfect solution.\n",
            "*   **Why Groq?** You get the power of a state-of-the-art model like **Llama 3 8B Instruct** without needing any special hardware. The speed is also mind-blowing, so your app will feel very responsive.\n",
            "*   **How to use it?** You'll sign up on the Groq website, get a free API key, and plug it into LangChain.\n",
            "\n",
            "---\n",
            "\n",
            "### Putting It All Together: A Simple Code Example\n",
            "\n",
            "This code snippet shows you how these pieces fit together. It's a complete, working RAG script.\n",
            "\n",
            "**Step 1: Get your environment ready**\n",
            "\n",
            "```bash\n",
            "# Create a folder for your project\n",
            "mkdir ai-research-companion\n",
            "cd ai-research-companion\n",
            "\n",
            "# Install all the necessary libraries\n",
            "pip install langchain langchain_community pypdf chromadb sentence-transformers langchain-groq\n",
            "```\n",
            "\n",
            "**Step 2: Get your API Key**\n",
            "1.  Go to `https://console.groq.com/`\n",
            "2.  Sign up and go to the \"API Keys\" section.\n",
            "3.  Create and copy your free API key.\n",
            "\n",
            "**Step 3: Write the Python Code**\n",
            "\n",
            "Create a file named `app.py` and paste this in. Also, create a folder named `research_papers` and put a few PDF research papers inside it.\n",
            "\n",
            "```python\n",
            "import os\n",
            "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from langchain_community.vectorstores import Chroma\n",
            "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "from langchain_groq import ChatGroq\n",
            "from langchain.chains import create_retrieval_chain\n",
            "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
            "from langchain_core.prompts import ChatPromptTemplate\n",
            "\n",
            "# --- CONFIGURATION ---\n",
            "# 1. Set up your Groq API Key\n",
            "# Best practice: set this as an environment variable\n",
            "os.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY_HERE\" \n",
            "\n",
            "# 2. Define paths and model names\n",
            "PAPERS_DIR = \"research_papers\"\n",
            "CHROMA_DB_DIR = \"chroma_db\"\n",
            "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
            "\n",
            "# --- MAIN LOGIC ---\n",
            "\n",
            "# Initialize the LLM with Groq\n",
            "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0)\n",
            "\n",
            "# Initialize the embedding model\n",
            "# This runs locally on your CPU\n",
            "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
            "\n",
            "# Check if the vector database already exists\n",
            "if os.path.exists(CHROMA_DB_DIR):\n",
            "    print(\"Loading existing vector database...\")\n",
            "    vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)\n",
            "else:\n",
            "    print(\"Vector database not found. Creating a new one...\")\n",
            "    # 1. Load documents from the specified directory\n",
            "    loader = PyPDFDirectoryLoader(PAPERS_DIR)\n",
            "    docs = loader.load()\n",
            "    print(f\"Loaded {len(docs)} document pages.\")\n",
            "\n",
            "    # 2. Split documents into smaller chunks\n",
            "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "    splits = text_splitter.split_documents(docs)\n",
            "    print(f\"Split documents into {len(splits)} chunks.\")\n",
            "\n",
            "    # 3. Create and persist the vector database\n",
            "    vectorstore = Chroma.from_documents(\n",
            "        documents=splits,\n",
            "        embedding=embeddings,\n",
            "        persist_directory=CHROMA_DB_DIR\n",
            "    )\n",
            "    print(\"Vector database created successfully.\")\n",
            "\n",
            "# Create a retriever from the vector store\n",
            "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve top 3 most relevant chunks\n",
            "\n",
            "# Define the prompt template for the RAG chain\n",
            "# This tells the LLM how to use the retrieved context\n",
            "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
            "Answer the user's question based only on the following context.\n",
            "If the context doesn't contain the answer, state that you don't have enough information.\n",
            "Do not make up information.\n",
            "\n",
            "Context:\n",
            "{context}\n",
            "\n",
            "Question: {input}\n",
            "\n",
            "Answer:\n",
            "\"\"\")\n",
            "\n",
            "# Create the main RAG chain\n",
            "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
            "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
            "\n",
            "# --- ASK A QUESTION ---\n",
            "print(\"\\nAI Research Companion is ready. Ask a question!\")\n",
            "question = \"What is the main contribution of the 'Attention Is All You Need' paper?\"\n",
            "response = rag_chain.invoke({\"input\": question})\n",
            "\n",
            "print(\"\\nQuestion:\", question)\n",
            "print(\"Answer:\", response[\"answer\"])\n",
            "\n",
            "# You can also inspect the retrieved source documents\n",
            "# print(\"\\n--- Retrieved Context ---\")\n",
            "# for i, doc in enumerate(response[\"context\"]):\n",
            "#     print(f\"Source {i+1}: (Page {doc.metadata.get('page', 'N/A')})\")\n",
            "#     print(doc.page_content)\n",
            "#     print(\"-\" * 20)\n",
            "\n",
            "```\n",
            "\n",
            "This script will perform the entire RAG process and give you an answer grounded in the PDFs you provided. This is the perfect starting point for your project\n",
            "You: thank you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: You're very welcome, Sneha!\n",
            "\n",
            "I'm glad I could help you map out the plan for your AI Research Companion. It's a really exciting project, and you have a great, modern tech stack to start with.\n",
            "\n",
            "As you begin to build it out, remember that this is a journey. Start with the script we outlined, get it working, and then you can add more features.\n",
            "\n",
            "Feel free to reach out anytime you have more questions, whether it's about:\n",
            "\n",
            "*   Troubleshooting code errors.\n",
            "*   Building a simple user interface (using a tool like **Streamlit** or **Gradio** is a fantastic next step).\n",
            "*   Improving the quality of the answers.\n",
            "*   Understanding any of the concepts more deeply.\n",
            "\n",
            "Good luck with the development, and have fun with it\n",
            "You: bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user02\"\n",
        "while True:\n",
        "  user_input =input(\"You: \")\n",
        "  if user_input.lower() in [\"bye\", \"exit\"]:\n",
        "    print(\"Bot: Goodbye!\")\n",
        "    break\n",
        "\n",
        "  response =runnable.invoke(\n",
        "          {\"input\": HumanMessage(content=user_input)},\n",
        "          config={\"configurable\": {\"session_id\": session_id}}\n",
        "  )\n",
        "  print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq26BZ-qxHxt",
        "outputId": "1d60a538-6718-4515-986f-823630b1e715"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: i am sri\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hello Sri! It's nice to meet you.\n",
            "\n",
            "How can I help you today?\n",
            "You: i am thinking about a 21 day complete reset plan with a fast on every tuesday and friday i want to reset my routine,health and academics  can u suggest a plan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Of course, Sri. This is an excellent goal. A 21-day reset is a powerful way to build new habits and break old ones. The inclusion of fasting will add a layer of discipline and potential health benefits.\n",
            "\n",
            "Here is a comprehensive 21-day reset plan designed to integrate your goals for routine, health, and academics.\n",
            "\n",
            "---\n",
            "\n",
            "### **The Philosophy: Consistency Over Perfection**\n",
            "\n",
            "The goal for these 21 days is not to be perfect, but to be **consistent**. If you miss a workout or eat something off-plan, don't scrap the whole day. Acknowledge it and get right back on track with the next scheduled action. This is about building a foundation for the long term.\n",
            "\n",
            "---\n",
            "\n",
            "### **Part 1: The Core Weekly Schedule**\n",
            "\n",
            "This is the template you will follow for each of the three weeks. The key is the structure.\n",
            "\n",
            "| Day | Routine Focus | Health Focus | Academic Focus |\n",
            "| :--- | :--- | :--- | :--- |\n",
            "| **Monday** | **Plan & Prepare:** Set weekly goals. Prepare for Tuesday's fast. | **Nourish:** Eat clean, whole foods. Moderate-intensity exercise (45 mins). | **Deep Work:** Tackle the most challenging subject. 2-3 focused study blocks. |\n",
            "| **Tuesday** | **Discipline & Focus:** Stick to your fast. Maintain a calm, steady pace. | **FASTING DAY:** Hydrate extensively. Light activity only (walking, stretching). | **Review & Organize:** Lighter tasks. Review notes, organize files, plan future assignments. |\n",
            "| **Wednesday**| **Re-engage & Energize:** Break your fast gently. Re-engage with full energy. | **Refuel:** Healthy, nutrient-dense meals. Strength training or HIIT (30-45 mins). | **Active Learning:** Attend classes with full focus. Engage in discussions. 2-3 study blocks. |\n",
            "| **Thursday** | **Momentum:** Build on Wednesday's energy. Prepare for Friday's fast. | **Nourish:** Clean eating. Moderate-intensity exercise (Cardio, 45 mins). | **Problem Solving:** Work on practice problems, essays, or projects. 2-3 study blocks. |\n",
            "| **Friday** | **Discipline & Wind-Down:** Stick to your fast. Begin to wind down for the weekend. | **FASTING DAY:** Hydrate extensively. Light activity only (yoga, long walk). | **Consolidate:** Review the week's material. Create summary sheets. Light reading. |\n",
            "| **Saturday**| **Active Rest & Recharge:** Looser schedule, but maintain healthy habits. | **Recharge:** Break fast gently. Enjoy healthy, delicious food. Fun, active hobby (hike, sport).| **Skill Building / Catch-up:** 1-2 study blocks on a passion project or catching up. |\n",
            "| **Sunday** | **Reflect & Plan:** Review the past week. Plan the week ahead. | **Rest & Recovery:** Full rest day. Gentle stretching. Meal prep for the week. | **Planning:** Plan your academic schedule for the coming week. Set goals. |\n",
            "\n",
            "---\n",
            "\n",
            "### **Part 2: Detailed Breakdown by Pillar**\n",
            "\n",
            "Here's how to execute on each pillar:\n",
            "\n",
            "#### **Pillar 1: Reset Your ROUTINE**\n",
            "\n",
            "The goal is to create a predictable structure that your mind and body can rely on.\n",
            "\n",
            "*   **Morning Routine (Daily, 60-90 mins):**\n",
            "    *   **Wake Up:** 6:00 AM (or a consistent time that works for you). **No phone for the first hour.**\n",
            "    *   **Hydrate:** Drink a large glass of water immediately.\n",
            "    *   **Move:** 10-15 minutes of light stretching, yoga, or a short walk outside.\n",
            "    *   **Mindfulness:** 10 minutes of meditation or quiet reflection/journaling. Write down 3 things you're grateful for and your main goal for the day.\n",
            "    *   **Plan:** Quickly review your schedule for the day while having your healthy breakfast (on non-fasting days).\n",
            "\n",
            "*   **Evening Routine (Daily, 60 mins):**\n",
            "    *   **Digital Sunset:** No screens (phone, laptop, TV) for at least 60 minutes before bed.\n",
            "    *   **Prepare:** Lay out your clothes for the next day. Pack your bag.\n",
            "    *   **Wind Down:** Read a physical book, listen to calm music, or do some light stretching.\n",
            "    *   **Reflect:** Spend 5 minutes journaling about the day. What went well? What was a challenge?\n",
            "    *   **Bedtime:** Be in bed by 10:00 PM (or a consistent time that allows for 7-8 hours of sleep).\n",
            "\n",
            "#### **Pillar 2: Reset Your HEALTH**\n",
            "\n",
            "**Important Disclaimer:** Fasting is not suitable for everyone. Please consult with a doctor before starting, especially if you have any underlying health conditions.\n",
            "\n",
            "*   **Fasting Days (Tuesday & Friday):**\n",
            "    *   **What kind of fast?** You decide what \"fast\" means.\n",
            "        *   **Option A (Strict):** Water, black coffee, and herbal tea only.\n",
            "        *   **Option B (Intermittent):** A 16:8 or 18:6 fasting window. For example, you only eat between 12 PM and 8 PM.\n",
            "        *   **Option C (Calorie-Restricted):** A very low-calorie day (under 500 calories) from clean sources like vegetable broth or a small protein shake.\n",
            "    *   **Hydration is KEY:** Aim for 3 liters of water throughout the day.\n",
            "    *   **Listen to your body:** If you feel very unwell, dizzy, or weak, break the fast with something light.\n",
            "    *   **Breaking the Fast:** Don't jump into a huge meal. Break it gently the next morning (or at your first meal) with something like a small portion of fruit, yogurt, bone broth, or a light smoothie.\n",
            "\n",
            "*   **Non-Fasting Days (The other 5 days):**\n",
            "    *   **The 80/20 Rule:** 80% of your food should be whole, unprocessed foods. 20% can be more flexible.\n",
            "    *   **Focus on:**\n",
            "        *   **Lean Protein:** Chicken, fish, eggs, tofu, lentils.\n",
            "        *   **Vegetables:** Fill half your plate with colorful veggies.\n",
            "        *   **Complex Carbs:** Brown rice, quinoa, oats, sweet potatoes.\n",
            "        *   **Healthy Fats:** Avocado, nuts, seeds, olive oil.\n",
            "    *   **Eliminate/Drastically Reduce:**\n",
            "        *   Sugary drinks and snacks.\n",
            "        *   Highly processed foods (anything in a crinkly packet).\n",
            "        *   Excessive caffeine and alcohol.\n",
            "\n",
            "*   **Exercise:**\n",
            "    *   Follow the weekly schedule. The mix of strength, cardio, and light activity is crucial.\n",
            "    *   On fasting days, a 30-45 minute walk is perfect. It aids the fast and keeps you active without depleting your energy.\n",
            "\n",
            "#### **Pillar 3: Reset Your ACADEMICS**\n",
            "\n",
            "*   **The Pomodoro Technique:** This will be your best friend.\n",
            "    *   Set a timer for 25 minutes.\n",
            "    *   Work on a single task with ZERO distractions (phone in another room).\n",
            "    *   When the timer rings, take a 5-minute break.\n",
            "    *   After 4 \"Pomodoros,\" take a longer 15-30 minute break.\n",
            "    *   Aim for a set number of these sessions per day as outlined in the weekly plan.\n",
            "\n",
            "*   **Dedicated Study Space:** Keep a clean, organized desk. This space is for work only. No eating, no social media.\n",
            "\n",
            "*   **Weekly Planning (Sunday):**\n",
            "    *   Open your calendar and syllabus for every class.\n",
            "    *   Map out all major deadlines and exams for the next 3 weeks.\n",
            "    *   Break down large assignments into small, daily tasks.\n",
            "    *   **Example:** \"Write History Paper\" becomes -> Mon: Research sources, Tue: Outline paper, Wed: Write introduction, etc.\n",
            "\n",
            "*   **Active Recall:** At the end of each study day, close your books and try to write down or say out loud the key concepts you learned. This dramatically improves retention.\n",
            "\n",
            "---\n",
            "\n",
            "### **The 3-Week Journey**\n",
            "\n",
            "*   **Week 1: The Foundation (Days 1-7):**\n",
            "    *   **Focus:** Just showing up. Your main goal is to follow the schedule. Don't worry about intensity.\n",
            "    *   **Expect:** You might feel tired, especially on fast days, as your body adjusts. You might struggle to wake up early. This is normal. Push through gently.\n",
            "\n",
            "*   **Week 2: Optimization (Days 8-14):**\n",
            "    *   **Focus:** Now that the routine is familiar, increase the quality. Push a little harder in your workouts. Make your study sessions even more focused. Refine your diet.\n",
            "    *   **Expect:** Your energy levels should be stabilizing. The fasts will likely feel easier. You'll start feeling the mental clarity and physical lightness.\n",
            "\n",
            "*   **Week 3: Consolidation (Days 15-21):**\n",
            "    *   **Focus:** Solidifying these actions into true habits. Make it feel automatic.\n",
            "    *   **Reflect:** Notice how far you've come. What's working best? What do you want to carry forward after the 21 days?\n",
            "    *   **Expect:** You should feel significantly betterâ€”more energetic, focused, and in control. This is the payoff.\n",
            "\n",
            "Good luck, Sri! This is a fantastic investment in yourself. Keep a simple journal to track your progress and feelings. You can do this.\n",
            "You: actaully i have a very strict college gng timing i.e from 7:30 am to 7:30 pm so give a plan keeping that in mind\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Excellent clarification, Sri. That 7:30 AM to 7:30 PM schedule is a game-changer and requires a much more strategic and realistic approach. A 12-hour commitment is incredibly demanding.\n",
            "\n",
            "The key to success here is not to add more stress, but to create a highly efficient structure around your fixed college hours. We will focus on **powerful bookends** to your day (morning and evening routines) and maximizing your weekends.\n",
            "\n",
            "Let's build a new, ultra-realistic 21-day reset plan.\n",
            "\n",
            "---\n",
            "\n",
            "### **The Core Philosophy: Efficiency and Recovery**\n",
            "\n",
            "With a 12-hour day, your energy is a precious resource. This plan is designed to maximize it, not deplete it. We will focus on:\n",
            "*   **Morning Activation:** Setting a positive and productive tone for the day.\n",
            "*   **Evening Decompression & Preparation:** Winding down effectively and making the next morning easy.\n",
            "*   **Weekend Reset:** Using the weekends for deeper recovery, planning, and bigger tasks.\n",
            "\n",
            "---\n",
            "\n",
            "### **Part 1: The New Weekly Schedule (7:30 AM - 7:30 PM College)**\n",
            "\n",
            "| Day | Morning Routine (5:30-7:00 AM) | Evening Routine (7:30-10:30 PM) | Health Focus | Academic Focus |\n",
            "| :--- | :--- | :--- | :--- | :--- |\n",
            "| **Mon** | **Energize:** 20-min HIIT or Bodyweight Circuit. | **Academic Focus:** 60-min homework/review block. | **Nourish:** Eat clean, whole foods. Pack a healthy lunch/snacks. | **Engage:** Actively participate in all classes. |\n",
            "| **Tue** | **Mindful Start:** 15-min walk/stretch. 10-min meditation. | **Light Tasks:** Organize notes, pack bag. Early night. | **FASTING DAY (Modified):** See fasting section below. Hydrate all day. | **Review:** Use free moments at college to review flashcards/notes. |\n",
            "| **Wed** | **Energize:** 20-min HIIT or Bodyweight Circuit. | **Academic Focus:** 60-min problem-solving/assignment block. | **Refuel:** Break fast gently (if applicable). Nutrient-dense meals. | **Deepen:** Clarify any confusing concepts from the first half of the week. |\n",
            "| **Thu** | **Active Start:** 20-min cardio (jogging, jump rope). | **Plan & Prep:** 30-min academic planning. Meal prep for Friday/weekend. | **Nourish:** Clean eating. Focus on hydration before the fast. | **Consolidate:** Review notes from the week so far. |\n",
            "| **Fri** | **Mindful Start:** 15-min walk/stretch. 10-min meditation. | **Decompress:** Minimal academics. Relax, read, listen to music. | **FASTING DAY (Modified):** Hydrate. Focus on getting through the long day. | **Light Review:** Quick 15-min scan of the week's material. |\n",
            "| **Sat** | **Longer Workout:** 45-60 min Strength Training or a long run/hike. | **Social/Hobby:** Connect with friends/family or enjoy a hobby. | **Recharge & Refuel:** Break your fast. Enjoy healthy, delicious food. | **Deep Work:** 2-3 hour focused study block for major projects/essays. |\n",
            "| **Sun** | **Active Recovery:** Long walk, yoga, or stretching. | **Weekly Plan:** Plan your upcoming week's schedule, goals, and outfits. | **Meal Prep:** Prepare lunches and snacks for the week ahead. | **Academic Planning:** Map out the week's assignments and study schedule. |\n",
            "\n",
            "---\n",
            "\n",
            "### **Part 2: Detailed Breakdown for Your Schedule**\n",
            "\n",
            "#### **Pillar 1: Your \"Bookend\" ROUTINE**\n",
            "\n",
            "**The Ultra-Efficient Morning Routine (5:30 AM - 7:00 AM)**\n",
            "*   **5:30 AM:** Wake Up & Hydrate. Drink a full glass of water. No phone.\n",
            "*   **5:35 AM:** Movement (20-30 mins). Alternate between Energize days and Mindful days as per the schedule. This is non-negotiable for energy.\n",
            "*   **6:10 AM:** Shower & Get Ready.\n",
            "*   **6:30 AM:** Healthy Breakfast (on non-fasting days). Something you prepped Sunday, like overnight oats or a smoothie.\n",
            "*   **6:45 AM:** Mindfulness & Planning (15 mins). 5 mins of quiet reflection/meditation. 10 mins to review your day's schedule and pack your bag.\n",
            "*   **7:00 AM:** Leave for college.\n",
            "\n",
            "**The Decompression Evening Routine (7:30 PM - 10:30 PM)**\n",
            "*   **7:30 PM:** Arrive Home. Put your things away immediately. Change into comfortable clothes.\n",
            "*   **7:45 PM:** Dinner. This should be a healthy meal you've prepped or can cook in under 20 minutes.\n",
            "*   **8:15 PM:** Decompression (15-30 mins). Sit in silence, stretch, talk with family. Do *not* jump straight into more work.\n",
            "*   **8:45 PM:** Focused Academic Block (60 mins on Mon/Wed). Use the Pomodoro Technique (2x 25-min blocks). This is for review and homework, not heavy new learning.\n",
            "*   **9:45 PM:** Digital Sunset & Wind Down. **No screens.** Prepare your clothes/bag for tomorrow. Read a book, journal, listen to calm music.\n",
            "*   **10:15 PM:** Final prep, brush teeth, etc.\n",
            "*   **10:30 PM:** In bed. Sleep is your most important recovery tool.\n",
            "\n",
            "#### **Pillar 2: Your HEALTH (Adapted for a Busy Schedule)**\n",
            "\n",
            "**Fasting on Tuesdays & Fridays - CRITICAL MODIFICATIONS**\n",
            "Fasting for a full day while attending 12 hours of college is extremely difficult and may impair your academic performance. I strongly recommend a modified approach:\n",
            "\n",
            "*   **Recommended Option: Intermittent Fasting (16:8).** This is the most sustainable.\n",
            "    *   **How it works:** You skip breakfast. Your eating \"window\" is from your lunch break (e.g., 1 PM) to your dinner (e.g., 9 PM).\n",
            "    *   **On Tuesday/Friday morning:** Have black coffee, herbal tea, or water.\n",
            "    *   **At college:** Pack a healthy, substantial lunch to break your fast. Hydrate constantly.\n",
            "*   **Alternative Option: Keep Fasts on Weekends.** If you find fasting on college days too draining, shift your fast days to Saturday or Sunday when you have more control over your energy and environment.\n",
            "\n",
            "**Nutrition is Non-Negotiable**\n",
            "*   **Sunday is Meal Prep Day:** This is the most important hour of your week.\n",
            "    *   Cook a big batch of quinoa or brown rice.\n",
            "    *   Grill or bake chicken/tofu.\n",
            "    *   Chop vegetables for salads or stir-fries.\n",
            "    *   Portion out lunches and healthy snacks (nuts, fruit, yogurt) for the week.\n",
            "*   **Pack Your Lunch & Snacks:** Never rely on buying food. It saves money, time, and ensures you eat healthily.\n",
            "\n",
            "#### **Pillar 3: Your ACADEMICS (Efficiency is Key)**\n",
            "\n",
            "*   **During College (7:30 AM - 7:30 PM): Be 100% Present.**\n",
            "    *   Since your free time is limited, you must maximize your time in class.\n",
            "    *   Take clear, concise notes. Ask questions. Eliminate distractions.\n",
            "    *   Use small 10-15 minute breaks between classes to quickly review notes from the previous lecture. This is a form of \"active recall\" and works wonders.\n",
            "\n",
            "*   **Evening Academic Block (Max 60-90 mins):**\n",
            "    *   **Purpose:** Not for learning new things, but for **consolidation**.\n",
            "    *   **Tasks:** Review the day's notes, finish small homework problems, plan the next day's study priorities.\n",
            "\n",
            "*   **Saturday Deep Work Block (2-3 hours):**\n",
            "    *   This is your time for war.\n",
            "    *   Tackle the most challenging tasks: writing a paper, working on a project, studying for a big exam.\n",
            "    *   Go to a library or a quiet space. Turn your phone off. This is an uninterrupted, focused session.\n",
            "\n",
            "This revised plan respects the reality of your demanding schedule, Sri. It turns your mornings and evenings into powerful rituals and leverages your weekends to do the heavy lifting.\n",
            "\n",
            "Remember the mantra: **Consistency over perfection.** If you have a tough day and can only manage a 10-minute walk in the morning, that is a WIN. Just stick to the structure. You can do this\n",
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user03\"\n",
        "while True:\n",
        "  user_input =input(\"You: \")\n",
        "  if user_input.lower() in [\"bye\", \"exit\"]:\n",
        "    print(\"Bot: Goodbye!\")\n",
        "    break\n",
        "\n",
        "  response =runnable.invoke(\n",
        "          {\"input\": HumanMessage(content=user_input)},\n",
        "          config={\"configurable\": {\"session_id\": session_id}}\n",
        "  )\n",
        "  print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWxeowKwygYm",
        "outputId": "c1d16c60-e8bd-4e49-b252-2b84a9fd2a75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: i am vaahani\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hello, Vaahani! It's nice to meet you.\n",
            "\n",
            "How can I help you today?\n",
            "You: what is this DataEconomy company?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Of course.\n",
            "\n",
            "**DataEconomy** is a global media, events, and market intelligence company focused specifically on the **business and finance of the data infrastructure industry**.\n",
            "\n",
            "Think of it as a specialized version of *Forbes* or *The Economist*, but dedicated entirely to the world of data centers, cloud computing, and the digital infrastructure that powers our online lives.\n",
            "\n",
            "Hereâ€™s a breakdown of what they do and who they are for:\n",
            "\n",
            "### Key Focus Areas:\n",
            "\n",
            "Their reporting and analysis cover the entire ecosystem of digital infrastructure, including:\n",
            "\n",
            "*   **Data Centers:** The physical buildings that house servers and data. This includes construction, operations, real estate, and colocation.\n",
            "*   **Cloud Computing:** The business side of public, private, and hybrid cloud services (like AWS, Google Cloud, Azure).\n",
            "*   **Edge Computing:** The trend of moving data processing closer to where it's generated (e.g., IoT devices, 5G towers).\n",
            "*   **Connectivity:** The networks that link everything together, like subsea cables and fiber optic networks.\n",
            "*   **Investment & Finance:** This is their core specialty. They track mergers and acquisitions (M&A), private equity deals, IPOs, and the financial performance of companies in this sector.\n",
            "*   **Sustainability:** The environmental impact of data centers, including power consumption and green energy initiatives.\n",
            "\n",
            "### What They Offer:\n",
            "\n",
            "*   **News Website & Digital Magazine:** Their primary platform (`data-economy.com`) provides daily news, feature articles, and interviews.\n",
            "*   **Print Magazine:** They publish a high-quality physical magazine that is distributed globally.\n",
            "*   **Events and Conferences:** They are well-known for hosting major industry events, such as the **Data Economy Global Awards** and the **Finvest Global Summit**, which bring together executives and investors.\n",
            "*   **Market Intelligence:** They produce reports and analysis on industry trends.\n",
            "\n",
            "### Their Target Audience:\n",
            "\n",
            "DataEconomy is not for the average consumer. Their content is created for:\n",
            "\n",
            "*   **C-Level Executives:** CEOs, CIOs, CTOs, and CFOs of tech and data center companies.\n",
            "*   **Investors:** Private equity firms, venture capitalists, and investment bankers who fund the industry.\n",
            "*   **Data Center Operators:** The people who build and run data centers.\n",
            "*   **Enterprise IT Leaders:** Decision-makers at large companies who need to buy cloud or data center services.\n",
            "\n",
            "**In summary:** If a company is building, buying, selling, or investing in the physical infrastructure that stores and moves data, **DataEconomy** is a primary source of news, analysis, and networking for them.\n",
            "You: skills one needd to be recurited it this company are?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Excellent question. To be recruited by a specialized company like **DataEconomy**, you'd need a combination of core professional skills and, most importantly, specific industry knowledge.\n",
            "\n",
            "Hereâ€™s a breakdown of the skills needed, categorized by the most likely job roles at the company.\n",
            "\n",
            "### Core Skills Required Across All Roles\n",
            "\n",
            "These are the foundational skills that would make you a strong candidate for any position at DataEconomy:\n",
            "\n",
            "1.  **Passion for the Industry:** A genuine interest in data centers, cloud computing, finance, and technology. You need to \"speak the language.\"\n",
            "2.  **Excellent Communication:** Whether written or verbal, the ability to communicate complex topics clearly and professionally is paramount.\n",
            "3.  **Professionalism and Polish:** You will be interacting with C-level executives and high-profile investors. Poise and confidence are key.\n",
            "4.  **Networking Ability:** A natural ability to build and maintain professional relationships.\n",
            "5.  **Self-Motivated and Proactive:** As a specialized media company, employees are often expected to be experts who can manage their own work and find new opportunities.\n",
            "\n",
            "---\n",
            "\n",
            "### Skills by Specific Job Type\n",
            "\n",
            "#### 1. For Journalism and Editorial Roles (e.g., Journalist, Editor, Reporter)\n",
            "\n",
            "This is the heart of their business. They need people who can create credible, insightful content.\n",
            "\n",
            "*   **Hard Skills:**\n",
            "    *   **Exceptional Writing and Editing:** The ability to write clean, compelling, and accurate news stories and feature articles under tight deadlines.\n",
            "    *   **Financial Literacy:** This is their unique angle. You **must** understand concepts like Mergers & Acquisitions (M&A), IPOs, private equity, venture capital, valuations, and how to read financial statements.\n",
            "    *   **Investigative and Research Skills:** The ability to dig deep into topics, find unique data points, and identify trends before others do.\n",
            "    *   **Interviewing Skills:** Confidence and skill in interviewing senior executives and asking insightful questions.\n",
            "    *   **Source Building (Rolodex):** A pre-existing network of contacts in the tech, finance, or data center industry is a massive advantage.\n",
            "\n",
            "*   **Soft Skills:**\n",
            "    *   Curiosity and a \"nose for news.\"\n",
            "    *   Integrity and commitment to journalistic ethics.\n",
            "    *   Ability to work under pressure.\n",
            "\n",
            "#### 2. For Sales and Business Development Roles (e.g., Sales Manager, Account Executive)\n",
            "\n",
            "These roles focus on selling advertising, event sponsorships, and market intelligence reports.\n",
            "\n",
            "*   **Hard Skills:**\n",
            "    *   **B2B Sales Experience:** Proven track record of selling high-value services to other businesses, not consumers.\n",
            "    *   **Consultative Selling:** The ability to understand a client's business needs and position DataEconomy's offerings (e.g., a sponsorship package) as a solution.\n",
            "    *   **Industry Knowledge:** You cannot effectively sell to a data center company or an investment firm if you donâ€™t understand their business, challenges, and goals.\n",
            "    *   **CRM Software Proficiency:** Experience with tools like Salesforce or HubSpot.\n",
            "\n",
            "*   **Soft Skills:**\n",
            "    *   Resilience and tenacity.\n",
            "    *   Strong negotiation and persuasion skills.\n",
            "    *   Excellent relationship-building capabilities.\n",
            "\n",
            "#### 3. For Events Roles (e.g., Events Manager, Conference Producer)\n",
            "\n",
            "These roles involve planning and executing their high-profile summits and awards ceremonies.\n",
            "\n",
            "*   **Hard Skills:**\n",
            "    *   **Project Management:** Meticulous organizational skills, with the ability to manage budgets, timelines, vendors, and logistics simultaneously.\n",
            "    *   **Content Curation / Agenda Setting:** The ability to identify hot topics and recruit influential speakers (CEOs, investors) to build a compelling event agenda.\n",
            "    *   **Vendor and Sponsor Management:** Experience coordinating with venues, AV teams, and ensuring sponsor deliverables are met.\n",
            "    *   **Event Management Software:** Familiarity with platforms like Cvent, Bizzabo, or similar.\n",
            "\n",
            "*   **Soft Skills:**\n",
            "    *   Grace under pressure and problem-solving on the fly.\n",
            "    *   Strong interpersonal skills for managing speakers and VIPs.\n",
            "\n",
            "### How to Stand Out as a Candidate\n",
            "\n",
            "*   **Demonstrate Your Knowledge:** Don't just say you're interested. In your cover letter or interview, reference a recent article they published or a major M&A deal in the data center space (e.g., \"I was fascinated by the recent KKR acquisition of a portfolio in Europe and how you covered it...\").\n",
            "*   **Follow Them and Their Staff:** Engage with DataEconomy and their key journalists (like JoÃ£o Marques Lima, the founder) on LinkedIn. Show you are part of the conversation.\n",
            "*   **Build a Portfolio:** If you're a writer, start a blog or write guest posts about the data infrastructure industry. If you're in sales, be prepared to discuss how you would identify and approach potential sponsors in their ecosystem.\n",
            "*   **Highlight Financial Acumen:** No matter the role, emphasizing any experience or understanding of investment and finance will make you a much stronger candidate, as this is their core differentiator.\n",
            "You: any often for a datascience graduate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: That's an insightful question. For a data science graduate, opportunities at a specialized media company like DataEconomy would be **niche but potentially very high-impact.**\n",
            "\n",
            "While they aren't a tech company that hires data scientists by the dozen, the value a data science graduate could bring is immense. Hereâ€™s a breakdown of the challenges and the significant opportunities.\n",
            "\n",
            "### The Challenge: They Aren't a Traditional Tech Employer\n",
            "\n",
            "*   **Limited Roles:** A company of this size likely doesn't have a large, dedicated \"Data Science\" team. You won't find dozens of open positions. The role might not even be called \"Data Scientist\"; it could be \"Data Analyst,\" \"Market Intelligence Analyst,\" or \"Data Journalist.\"\n",
            "*   **Domain Knowledge is King:** They would likely value a candidate with deep knowledge of finance and data centers *over* a candidate with perfect Python skills but no industry context. You need to bridge that gap.\n",
            "\n",
            "### The Huge Opportunity: Where a Data Scientist Could Be a Game-Changer\n",
            "\n",
            "DataEconomy's entire business is about turning information into valuable intelligence. A data scientist can supercharge this process in several key areas.\n",
            "\n",
            "#### 1. Market Intelligence & Research (This is the #1 Opportunity)\n",
            "\n",
            "This is their core business beyond news. They create reports and analysis that people pay for. A data scientist could:\n",
            "\n",
            "*   **Build Predictive Models:** Create models to forecast data center market growth, energy consumption, or investment trends.\n",
            "*   **Analyze Financial Data:** Scrape and analyze SEC filings, M&A announcements, and private equity deals to find patterns that traditional journalists might miss. For example, \"What is the average valuation multiple for a data center in Europe vs. the US?\"\n",
            "*   **Create Proprietary Datasets:** Develop unique, sellable datasets. For instance, track and analyze every data center being built globally, including its power capacity, location, and owner. This dataset itself becomes a product.\n",
            "*   **Master Data Visualization:** Go beyond simple charts. Create interactive dashboards and compelling visualizations for their articles and reports that make complex data easy to understand.\n",
            "\n",
            "#### 2. Data-Driven Journalism\n",
            "\n",
            "This is the future of media. Instead of just reporting on a press release, a data journalist finds the story within the data.\n",
            "\n",
            "*   **Trend Spotting:** Use NLP (Natural Language Processing) to analyze thousands of industry news articles, earnings call transcripts, or social media posts to identify emerging trends or shifts in sentiment.\n",
            "*   **Investigative Stories:** Analyze public datasets (e.g., power grid usage, land registries, job postings) to uncover stories, such as which tech giant is secretly planning a new data center campus.\n",
            "*   **Automated Data Gathering:** Build scripts to automatically collect key industry metrics, freeing up journalists to focus on analysis and storytelling.\n",
            "\n",
            "#### 3. Audience Growth and Engagement\n",
            "\n",
            "*   **Audience Segmentation:** Analyze website reader data to understand what content resonates most with different segments of their audience (e.g., investors vs. engineers).\n",
            "*   **Content Personalization:** Help develop systems to recommend relevant articles to readers, increasing engagement and subscription conversions.\n",
            "\n",
            "### How to Position Yourself as a Data Science Graduate\n",
            "\n",
            "Because the roles are scarce, you need to be a perfect fit. Here is your roadmap:\n",
            "\n",
            "1.  **Become Obsessed with the Domain:** This is non-negotiable.\n",
            "    *   Read DataEconomy, Capacity Media, and other industry publications daily.\n",
            "    *   Learn the key players: Digital Realty, Equinix, KKR, Blackstone, AWS, Google Cloud.\n",
            "    *   Understand the financial jargon: M&A, private equity, REITs, CAPEX, EBITDA.\n",
            "\n",
            "2.  **Build a Killer Portfolio Project:** This is your key to getting an interview. Do not use the standard Titanic or Iris datasets. Create a project that shows you can solve *their* problems.\n",
            "    *   **Project Idea:** Scrape data on all major data center M&A deals announced in the last three years. Analyze the data to show trends in deal size, valuation multiples, and which investors are most active. Create a compelling blog post or interactive dashboard with your findings. **This single project would make you an incredibly attractive candidate.**\n",
            "\n",
            "3.  **Master Data Storytelling:** You can't just present a chart; you must explain what it means. Practice turning your data findings into a clear, concise narrative. Your skills are not just in the analysis, but in communicating the \"so what?\"\n",
            "\n",
            "4.  **Highlight Hybrid Skills:** Frame yourself as a \"Data Analyst for Digital Infrastructure\" or a \"Quantitative Researcher for Tech Finance.\" Show that you bridge the gap between data science and their specific industry.\n",
            "\n",
            "**In summary:** A standard data science graduate might be overlooked. But a data science graduate who has proactively learned the business of data centers and finance, and who can demonstrate how their skills can generate unique market intelligence, would be an **extremely powerful and sought-after candidate** for a company like DataEconomy.\n",
            "You: bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U74lkXEk0rBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}